groups:
  - name: service_health
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High 5xx error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has >5% error rate (current: {{ $value | humanizePercentage }})"

      - alert: HighLatencyP99
        expr: |
          histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))
          > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High p99 latency on {{ $labels.job }}"
          description: "{{ $labels.job }} p99 latency >500ms (current: {{ $value | humanizeDuration }})"

      - alert: HighLatencyP99Critical
        expr: |
          histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))
          > 2.0
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Critical p99 latency on {{ $labels.job }}"
          description: "{{ $labels.job }} p99 latency >2s (current: {{ $value | humanizeDuration }})"

      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been unreachable for >1 minute."

  - name: pod_health
    rules:
      - alert: PodRestarting
        expr: |
          increase(kube_pod_container_status_restarts_total[1h]) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} restarting frequently"
          description: "Pod {{ $labels.pod }} restarted {{ $value }} times in the last hour."

      - alert: PodNotReady
        expr: |
          kube_pod_status_ready{condition="true"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is not ready"
          description: "Pod {{ $labels.pod }} has been in not-ready state for >5 minutes."

  - name: resource_usage
    rules:
      - alert: HighCPUUsage
        expr: |
          (
            sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
            /
            sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod)
          ) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.pod }}"
          description: "{{ $labels.pod }} CPU usage >85% for >10 minutes."

      - alert: HighMemoryUsage
        expr: |
          (
            sum(container_memory_working_set_bytes) by (pod)
            /
            sum(kube_pod_container_resource_limits{resource="memory"}) by (pod)
          ) > 0.90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "{{ $labels.pod }} memory usage >90% â€” risk of OOMKill."

  - name: database
    rules:
      - alert: PostgreSQLHighConnections
        expr: |
          pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL connection count high"
          description: "{{ $value }} active connections (max default is 100)."

      - alert: PostgreSQLSlowQueries
        expr: |
          rate(pg_stat_activity_max_tx_duration{datname!~"template.*"}[5m]) > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL has long-running transactions"
          description: "Transactions running >5 minutes on {{ $labels.datname }}."

  - name: messaging
    rules:
      - alert: RabbitMQQueueDepth
        expr: |
          rabbitmq_queue_messages > 10000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "RabbitMQ queue depth high: {{ $labels.queue }}"
          description: "Queue {{ $labels.queue }} has {{ $value }} messages backed up."

      - alert: RabbitMQDLQMessages
        expr: |
          rabbitmq_queue_messages{queue=~".*\\.dlq"} > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Messages in DLQ: {{ $labels.queue }}"
          description: "{{ $value }} messages in dead letter queue {{ $labels.queue }}."

      - alert: KafkaConsumerLag
        expr: |
          kafka_consumer_group_lag > 50000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Kafka consumer lag high for {{ $labels.group }}"
          description: "Consumer group {{ $labels.group }} has lag of {{ $value }} on {{ $labels.topic }}."

  - name: redis
    rules:
      - alert: RedisHighMemoryUsage
        expr: |
          redis_memory_used_bytes / redis_memory_max_bytes > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage >85%"
          description: "Redis memory at {{ $value | humanizePercentage }} of max."

      - alert: RedisHighConnectionCount
        expr: |
          redis_connected_clients > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis connection count high"
          description: "{{ $value }} clients connected to Redis."
